    Active set method solves general quadratic program (QP) in the following form:
    \begin{subequations}
        \label{prob: qp}
        \begin{align}
            \min_x q(x) = &\frac{1}{2} x^TGx + x^Tc \tag{16.1a}   \label{exp: qp_obj} \\
            \text{subject to \quad}  &a_i^Tx = b_i, i \in \mathcal{E} \tag{16.1b} \label{con: equality} \\
            &a_i^Tx \geq b_i, i \in \mathcal{I}  \tag{16.1c} \label{con: inequality}
%        \label{prob: 16.1}
        \end{align}
    \end{subequations}


    where $G$ is a symmetric $n\times n$ matrix, $\mathcal{E}$ and $\mathcal{I}$ are finite sets of indices, and $c, x$ and
    $\{a_i\}, i\in\mathcal{E}\cup\mathcal{I}$ are vectors in $\mathbb{R}^n$.
    We say that problem\eqref{prob: qp} is convex if matrix $G$ is positive semi definite.

    \subsection{Equality constrained Quadratic Programs} \label{subsec:-ec_qp}
    \begin{subequations}
        \label{prob: ec_qp}
        \begin{align}
            \min_x q(x) = &\frac{1}{2} x^TGx + x^Tc \tag{16.3a}   \label{exp: ec_qp_obj} \\
            \text{subject to \quad}  &Ax = b \tag{16.3b} \label{con: ec_qp}
        \end{align}
    \end{subequations}
    where A is the $m\times n$ matrix whose rows are $a_i^T, i\in\mathcal{E}$ and $b$ is the vector in $\mathcal{R}^m$
    whose components are $b_i, i\in\mathcal{I}$.\\
    The Lagrangian function of (16.3) is:
    \begin{align*}
        \mathcal{L}(x^*, \lambda^*) = \frac{1}{2} x^TGx + x^Tc - \lambda^{*T}(Ax - b)
    \end{align*}
    The first order necessary condition for $x^*$ to be a solution of(16.3) states that there is a vector $\lambda^*$
    such that:
    \begin{align*}
        Gx^* + c - A^T\lambda^* = 0\\
        Ax^* = b
    \end{align*}

    In matrix form:
    \[
        \begin{bmatrix}
            &G & -A^T& \\
            &A & 0&
        \end{bmatrix}
        \begin{bmatrix}
            &x^*&\\
            &\lambda^*&
        \end{bmatrix} =
        \begin{bmatrix}
            &-c&\\
            &b&
        \end{bmatrix}\tag{16.4} \label{KKT: original matrix}
    \]

    Suppose that the search direction is $p$ and $x^* = x + p $, the KKT condition becomes
    \begin{align*}
        G(x + p) + c - A^T\lambda^* = 0\\
        A(x + p) = b
    \end{align*}

    Rearranging the equations, we obtain

    \[
        \begin{bmatrix}
            &G & A^T& \\
            &A & 0&
        \end{bmatrix}
        \begin{bmatrix}
            &-p&\\
            &\lambda^*&
        \end{bmatrix} =
        \begin{bmatrix}
            &g&\\
            &h&
        \end{bmatrix}\tag{16.5} \label{KKT: step matrix}
    \]

    where
    \begin{align}
        h = Ax -b,  \quad g = c + Gx, \quad p = x^* - x \tag{16.6}\label{KKT: transform}
    \end{align}
    The matrix in \eqref{KKT: step matrix} is called the KKT matrix and lemma 16.1 gives that
    \[
        K =
        \begin{bmatrix}
            &G & A^T& \\
            &A & 0&
        \end{bmatrix}
    \]
    is non-singular.
    Z denotes the $n\times(n -m)$ matrix whose columns are a basis for the null space of A. That is
    Z has full rank and satisfies $AZ = 0$

    \subsection{Direct Solution of the KKT System} \label{subsec:-direct-solution-KKT}

    \subsubsection{Symmetric indefinite factorization} \label{subsubsec:-symmetric-indefinite-factorization}
    \begin{align}
        P^TKP = LBL^T \tag{16.12} \label{eqn: symmetric-indefinite-factorization}
    \end{align}

    \subsection{Schur-Complement Method} \label{subsec:-schur-complement-method}

    \subsection{Null-Space Method} \label{subsec:-null-space-method}
    \par The null space method does not require non-singularity of G and therefore has wider applicability than the
    Schur-complement method.It assumes only that $A$ has full row rank and that $Z^TGZ$ is positive definite.However,
    it requires knowledge of the null-space basis matrix Z. Like the Schur-complement method, it exploits the block
    structure in the KKT system to decouple\eqref{KKT: step matrix} into two smaller systems.
    \par Suppose that we partition the vector $p$ in\eqref{KKT: step matrix} into two components:
    \begin{align}
        p = Yp_Y + Zp_Z \tag{16.17} \label{eqn: 16.17}
    \end{align}

    By substituting $p$ into the second equation of\eqref{KKT: step matrix} we obtain:
    \begin{align*}
        Ap = -h\\
        A(Yp_Y + Zp_Z) = -h\\
        AYp_Y + AZp_Z = -h\\
        AYp_Y = -h \tag{16.18} \label{eqn: 16.18}\\
        p_Y = -(AY)^{-1}h
    \end{align*}

    Meanwhile, we can substitute\eqref{eqn: 16.17} into the first equation of\eqref{KKT: step matrix} to obtain:
    \begin{align}
        -GYp_Y - GZp_Z + A^T\lambda^* = g
    \end{align}
    and multiply by $Z^T$ to obtain
    \begin{align}
        (Z^T GZ)p_{Z} = -Z^T GYp_Y - Z^T g
    \end{align}
    This system can be solved by performing a Cholesky factorization of the reduced Hessian matrix $Z^TGZ$
    to determine $p_Z$.We therefore can compute the total step $p$.
    \par To obtain the Lagrange multiplier, we multiply the first block row in\eqref{KKT: step matrix} by $Y^T$
    to obtain the linear system
    \begin{align*}
        A^T\lambda^* = g + Gp \\
        (AY)^T\lambda^* = Y^T(g + Gp) \tag{16.20}\label{eqn: 16.20}\\
        \lambda^* = (AY)^{-1}Y^T(g + Gp)
    \end{align*}

    \subsection{Updating Factorizations} \label{subsec:-updating-factorizations}
    \[
        A^T\Pi = Q
        \begin{bmatrix}
            & R & \\
            & 0 &
        \end{bmatrix} =
        \begin{bmatrix}
            & Q_1 & Q_2 &
        \end{bmatrix}
        \begin{bmatrix}
            & R & \\
            & 0 &
        \end{bmatrix}\tag{16.50}\label{eqn: 16.50}
    \]
    where $\Pi$ is a permutation matrix; $R$ is square, upper triangular and non-singular;
     $   Q =
        \begin{bmatrix}
            & Q_1 & Q_2 &
        \end{bmatrix}
    $
    is $n\times n$ orthogonal and $Q_1, R$ both have $m$ columns while $Q_2$ has $n - m$ columns.
